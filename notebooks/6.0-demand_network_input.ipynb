{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Demand data, so it can be used by a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import spatial\n",
    "from typing import Tuple"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T08:48:45.109756Z",
     "start_time": "2023-05-09T08:48:42.198898Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Demand Data and Passenger objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T08:49:09.489358Z",
     "start_time": "2023-05-09T08:49:06.614001Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.robin.demand.entities import Demand, Passenger\n",
    "\n",
    "path_config_demand = '../configs/test_case/demand_data.yml'\n",
    "\n",
    "demand = Demand.from_yaml(path_config_demand)\n",
    "\n",
    "passengers = demand.generate_passengers()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get passenger relevant information and save it in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5956, 5)\n",
      "  user_pattern origin destination     arrival_datetime  \\\n",
      "0      student  60000       04040  [2023, 6, 1, 14, 4]   \n",
      "1     business  60000       71801  [2023, 6, 1, 7, 48]   \n",
      "2     business  60000       71801  [2023, 6, 1, 8, 48]   \n",
      "3     business  60000       71801  [2023, 6, 1, 8, 11]   \n",
      "4     business  60000       04040  [2023, 6, 1, 8, 37]   \n",
      "\n",
      "                                         scaled_data  \n",
      "0  [0.0, 0.0, 0.0, 0.6086956521739131, 0.06779661...  \n",
      "1  [0.0, 0.0, 0.0, 0.30434782608695654, 0.8135593...  \n",
      "2  [0.0, 0.0, 0.0, 0.34782608695652173, 0.8135593...  \n",
      "3  [0.0, 0.0, 0.0, 0.34782608695652173, 0.1864406...  \n",
      "4  [0.0, 0.0, 0.0, 0.34782608695652173, 0.6271186...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_passenger_info(passenger: Passenger) -> Tuple[str, str, str, datetime.datetime]:\n",
    "    \"\"\"\n",
    "    Get the information of a passenger and return it as a tuple\n",
    "\n",
    "    Args:\n",
    "        passenger: Passenger object\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str, str, datetime.date, float]: Tuple with the information of the passenger\n",
    "    \"\"\"\n",
    "    user_pattern = passenger.user_pattern.name\n",
    "    origin, destination = passenger.market.departure_station, passenger.market.arrival_station\n",
    "    arrival_day = datetime.datetime.combine(passenger.arrival_day.date, datetime.datetime.min.time())\n",
    "    arrival_time = passenger.arrival_time\n",
    "    arrival_hour = int(arrival_time)\n",
    "    arrival_minutes = int((arrival_time - arrival_hour) * 60)\n",
    "    arrival_datetime = arrival_day + datetime.timedelta(hours=arrival_hour, minutes=arrival_minutes)\n",
    "    return user_pattern.lower(), origin, destination, arrival_datetime\n",
    "\n",
    "# Map a list of Passenger objects to a list of tuples with the passenger information\n",
    "passengers_info = list(map(get_passenger_info, passengers))\n",
    "\n",
    "def get_datetime_vector(datetime: datetime.datetime) -> np.array:\n",
    "    \"\"\"\n",
    "    Returns a vector with the datetime information\n",
    "\n",
    "    Args:\n",
    "        datetime (datetime.datetime): Datetime object\n",
    "\n",
    "    Returns:\n",
    "        np.array: Vector with the datetime information\n",
    "    \"\"\"\n",
    "    return np.array([datetime.year, datetime.month, datetime.day, datetime.hour, datetime.minute])\n",
    "\n",
    "# Dataframe with the passenger information\n",
    "df = pd.DataFrame(passengers_info, columns=['user_pattern', 'origin', 'destination', 'arrival_datetime'])\n",
    "df['arrival_datetime'] = df['arrival_datetime'].apply(get_datetime_vector)\n",
    "arrival_dates = np.array(df['arrival_datetime'].tolist())\n",
    "print(arrival_dates.shape)\n",
    "scaled_data = MinMaxScaler().fit_transform(arrival_dates)\n",
    "df['scaled_data'] = scaled_data.tolist()\n",
    "# Padd scaled data column\n",
    "df['scaled_data'] = df['scaled_data'].apply(lambda x: np.array(x + [0.0] * (50 - len(x))))\n",
    "\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:53:33.355485Z",
     "start_time": "2023-05-09T09:53:33.293314Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Map stations IDs to Stations names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_pattern  origin destination     arrival_datetime  \\\n",
      "0      student  madrid    zaragoza  [2023, 6, 1, 14, 4]   \n",
      "1     business  madrid   barcelona  [2023, 6, 1, 7, 48]   \n",
      "2     business  madrid   barcelona  [2023, 6, 1, 8, 48]   \n",
      "3     business  madrid   barcelona  [2023, 6, 1, 8, 11]   \n",
      "4     business  madrid    zaragoza  [2023, 6, 1, 8, 37]   \n",
      "\n",
      "                                         scaled_data  \n",
      "0  [0.0, 0.0, 0.0, 0.6086956521739131, 0.06779661...  \n",
      "1  [0.0, 0.0, 0.0, 0.30434782608695654, 0.8135593...  \n",
      "2  [0.0, 0.0, 0.0, 0.34782608695652173, 0.8135593...  \n",
      "3  [0.0, 0.0, 0.0, 0.34782608695652173, 0.1864406...  \n",
      "4  [0.0, 0.0, 0.0, 0.34782608695652173, 0.6271186...  \n"
     ]
    }
   ],
   "source": [
    "stations_csv_path = f'../data/renfe/renfe_stations.csv'\n",
    "\n",
    "def get_renfe_station_id(adif_id: str, stations_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Returns the Station name given the Adif station id.\n",
    "\n",
    "    Args:\n",
    "        adif_id (str): Adif station id.\n",
    "        stations_df (pd.DataFrame): Dataframe with the stations' information.\n",
    "\n",
    "    Returns:\n",
    "        str: Station name.\n",
    "    \"\"\"\n",
    "    station_name = stations_df[stations_df['stop_id'] == adif_id]['stop_name'].values[0]\n",
    "    station_name = station_name.replace(\"-\", \" \").split(\" \")[0].lower()\n",
    "    return station_name\n",
    "\n",
    "stations_df = pd.read_csv(stations_csv_path, dtype={'stop_id': str, 'renfe_id': str})\n",
    "\n",
    "df['origin'] = df['origin'].apply(get_renfe_station_id, args=(stations_df,))\n",
    "df['destination'] = df['destination'].apply(get_renfe_station_id, args=(stations_df,))\n",
    "\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:53:40.188944Z",
     "start_time": "2023-05-09T09:53:37.468536Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import pre-trained GloVe embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barcelona', 'business', 'madrid', 'zaragoza', 'student'}\n"
     ]
    }
   ],
   "source": [
    "# Get bag of words\n",
    "words_set = set(df[['user_pattern', 'origin', 'destination']].values.flatten())\n",
    "\n",
    "print(words_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:53:41.643073Z",
     "start_time": "2023-05-09T09:53:41.634872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "barcelona [ 0.68944   1.2217   -0.23655   0.36109  -0.62116  -1.0075   -0.52565\n",
      "  0.65766  -1.2764    1.1286    1.1386   -0.36088  -1.3849   -0.58442\n",
      "  0.9772   -0.35103   0.29237  -0.27426  -1.3109   -0.015967 -1.0695\n",
      "  0.11901  -0.56335   0.49648  -0.44571  -0.47566   0.79045   0.42923\n",
      " -0.76743  -0.14029   1.7552    1.3342   -0.42864  -0.29125  -0.2056\n",
      "  0.21055   0.099324  1.4187    0.34068  -0.4477    1.0795    0.10387\n",
      "  0.15772  -0.51013  -0.50933   0.25395   0.050859 -0.084172  0.69738\n",
      " -1.192   ]\n",
      "business [ 0.023693  0.13316   0.023131  0.49833   0.026874 -0.43252  -1.1364\n",
      " -0.82001   0.22388  -0.032119 -0.069651  0.39857  -0.58275   0.095008\n",
      " -0.023643  0.23237  -0.42441   0.65709   0.57802  -0.51602   1.8253\n",
      "  0.12951  -0.61773   0.39281  -0.35754  -1.6778   -0.45201  -0.47075\n",
      "  0.19487   0.35828   3.6034    0.32865   0.47288  -0.33787  -0.46234\n",
      " -0.51628  -1.3755    0.70789   0.4648   -0.16186  -0.0961   -0.28523\n",
      "  0.30047   0.50902   0.081356 -0.015639 -0.51021   0.34585   0.24201\n",
      "  0.82237 ]\n",
      "madrid [ 1.3315     0.72181   -0.060088   0.43948    0.18419   -1.5083\n",
      " -0.48125    0.46037   -1.4088     1.2701     0.68031   -0.59232\n",
      " -1.6325    -0.30376    0.87685   -0.75531   -0.37583   -0.5363\n",
      " -1.0669     0.45537   -0.66694    0.43001   -0.69525    0.67518\n",
      " -0.93783   -0.67933    1.1104     0.37576   -0.36894   -0.083185\n",
      "  2.0346     0.96286   -0.56629   -0.7787    -0.10705   -0.14102\n",
      "  0.07384    0.62338    0.20366    0.0076751  0.71088    0.01501\n",
      "  0.53186   -0.82256   -0.35087    0.30876   -0.065328   0.23722\n",
      "  1.4692    -0.93469  ]\n",
      "zaragoza [ 1.0642    0.089939 -0.28715   0.82471   0.31063  -1.4798    0.12028\n",
      "  0.7449   -1.2919    0.39737   0.4715   -0.53483  -0.48049  -1.2998\n",
      "  0.32826  -1.3085   -0.67916  -0.20625  -0.7232    0.41638  -1.2895\n",
      " -0.69963  -0.23631   0.70175  -0.61498   0.20193   1.329     0.25294\n",
      " -0.093715 -0.16535   0.99605   1.2007   -0.26729  -0.42035  -0.15881\n",
      "  0.63906  -0.73977   1.3119    0.61136  -0.59197   1.3032    0.16094\n",
      " -0.027686 -0.9412   -0.68288   0.65116   0.47976  -0.32516   1.3912\n",
      " -0.50467 ]\n",
      "student [-1.0729    0.94103   0.084904 -1.0766    0.42866   0.099877 -0.51081\n",
      " -0.24961  -0.30883   0.19553   0.1965   -0.73152   0.096916 -0.062686\n",
      "  0.12078  -0.72384  -0.382     0.6934    0.32956   0.40244   0.53485\n",
      "  0.91781  -0.44553   0.71804  -0.13635  -1.6906    0.15818  -1.2367\n",
      " -1.2278   -0.058566  2.7544    0.18672  -0.263    -1.2792    0.16992\n",
      "  0.40748   0.12248   0.11211   0.78318   0.036392 -0.40808  -0.058474\n",
      " -0.27932   0.33035   0.52384  -1.0487    0.27565   0.0363    0.048604\n",
      "  0.28239 ]\n"
     ]
    }
   ],
   "source": [
    "# Import GloVe embeddings 50D\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "word_index = {word: index for index, word in enumerate(words_set)}\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('../data/pretrained/glove6B/glove.6B.50d.txt'))\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=np.float32)\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "embedding_dict = {}\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    print(word, embedding_vector)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        embedding_dict[word] = embedding_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:53:47.189315Z",
     "start_time": "2023-05-09T09:53:43.664247Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-1.0729  ,  0.94103 ,  0.084904, -1.0766  ,  0.42866 ,  0.099877,\n",
      "       -0.51081 , -0.24961 , -0.30883 ,  0.19553 ,  0.1965  , -0.73152 ,\n",
      "        0.096916, -0.062686,  0.12078 , -0.72384 , -0.382   ,  0.6934  ,\n",
      "        0.32956 ,  0.40244 ,  0.53485 ,  0.91781 , -0.44553 ,  0.71804 ,\n",
      "       -0.13635 , -1.6906  ,  0.15818 , -1.2367  , -1.2278  , -0.058566,\n",
      "        2.7544  ,  0.18672 , -0.263   , -1.2792  ,  0.16992 ,  0.40748 ,\n",
      "        0.12248 ,  0.11211 ,  0.78318 ,  0.036392, -0.40808 , -0.058474,\n",
      "       -0.27932 ,  0.33035 ,  0.52384 , -1.0487  ,  0.27565 ,  0.0363  ,\n",
      "        0.048604,  0.28239 ], dtype=float32), array([ 1.3315   ,  0.72181  , -0.060088 ,  0.43948  ,  0.18419  ,\n",
      "       -1.5083   , -0.48125  ,  0.46037  , -1.4088   ,  1.2701   ,\n",
      "        0.68031  , -0.59232  , -1.6325   , -0.30376  ,  0.87685  ,\n",
      "       -0.75531  , -0.37583  , -0.5363   , -1.0669   ,  0.45537  ,\n",
      "       -0.66694  ,  0.43001  , -0.69525  ,  0.67518  , -0.93783  ,\n",
      "       -0.67933  ,  1.1104   ,  0.37576  , -0.36894  , -0.083185 ,\n",
      "        2.0346   ,  0.96286  , -0.56629  , -0.7787   , -0.10705  ,\n",
      "       -0.14102  ,  0.07384  ,  0.62338  ,  0.20366  ,  0.0076751,\n",
      "        0.71088  ,  0.01501  ,  0.53186  , -0.82256  , -0.35087  ,\n",
      "        0.30876  , -0.065328 ,  0.23722  ,  1.4692   , -0.93469  ],\n",
      "      dtype=float32), array([ 1.0642  ,  0.089939, -0.28715 ,  0.82471 ,  0.31063 , -1.4798  ,\n",
      "        0.12028 ,  0.7449  , -1.2919  ,  0.39737 ,  0.4715  , -0.53483 ,\n",
      "       -0.48049 , -1.2998  ,  0.32826 , -1.3085  , -0.67916 , -0.20625 ,\n",
      "       -0.7232  ,  0.41638 , -1.2895  , -0.69963 , -0.23631 ,  0.70175 ,\n",
      "       -0.61498 ,  0.20193 ,  1.329   ,  0.25294 , -0.093715, -0.16535 ,\n",
      "        0.99605 ,  1.2007  , -0.26729 , -0.42035 , -0.15881 ,  0.63906 ,\n",
      "       -0.73977 ,  1.3119  ,  0.61136 , -0.59197 ,  1.3032  ,  0.16094 ,\n",
      "       -0.027686, -0.9412  , -0.68288 ,  0.65116 ,  0.47976 , -0.32516 ,\n",
      "        1.3912  , -0.50467 ], dtype=float32), array([0.        , 0.        , 0.        , 0.60869565, 0.06779661,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ])]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "input_data = df[['user_pattern', 'origin', 'destination', 'scaled_data']].values.tolist()\n",
    "\n",
    "input_vectors = []\n",
    "for row in deepcopy(input_data):\n",
    "    row[:3] = list(map(lambda word: embedding_dict.get(word), row[:3]))\n",
    "    input_vectors.append(row)\n",
    "\n",
    "#input_vectors = np.array([np.array(list(map(lambda word: embeddings_index.get(word), row[:3])) + row[-1]) for row in input_data])\n",
    "\n",
    "print(input_vectors[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:53:48.377962Z",
     "start_time": "2023-05-09T09:53:48.334531Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.65050876 0.4465016  0.42221534 0.64011216]\n",
      " [0.9662026  0.7904266  0.0613178  0.22272074]\n",
      " [0.4362595  0.36590302 0.57302403 0.15569937]\n",
      " [0.6882864  0.7655716  0.90022624 0.9065237 ]\n",
      " [0.04508305 0.31924665 0.7474377  0.0195899 ]\n",
      " [0.20869064 0.4683851  0.63936234 0.99558187]\n",
      " [0.37097585 0.8990139  0.08087122 0.85237277]\n",
      " [0.79036653 0.8027264  0.2237761  0.6445254 ]\n",
      " [0.7936405  0.37244594 0.10876894 0.64354753]\n",
      " [0.78340375 0.5202061  0.36396265 0.04890037]\n",
      " [0.9221221  0.07489288 0.24913776 0.2682953 ]\n",
      " [0.41051102 0.54679406 0.5661305  0.60156333]\n",
      " [0.24146652 0.7910892  0.14174306 0.29268622]\n",
      " [0.5695696  0.41060877 0.17198431 0.47137773]\n",
      " [0.77031446 0.6005255  0.6517365  0.59011984]\n",
      " [0.75613904 0.25892174 0.98167837 0.24066949]\n",
      " [0.37523973 0.8791685  0.01619399 0.49088526]\n",
      " [0.26744783 0.6887306  0.67361367 0.55209637]\n",
      " [0.5478947  0.33871293 0.06698322 0.38533616]\n",
      " [0.9255959  0.9520111  0.12218833 0.6654377 ]\n",
      " [0.83597565 0.9185085  0.5610728  0.7742789 ]\n",
      " [0.88293016 0.453897   0.89888084 0.947634  ]\n",
      " [0.59688735 0.08956289 0.51163006 0.6385218 ]\n",
      " [0.92942    0.15306377 0.19177556 0.01104867]\n",
      " [0.32068264 0.32896912 0.01025379 0.07268822]\n",
      " [0.38681364 0.43398964 0.5124346  0.6307069 ]\n",
      " [0.25461483 0.67414975 0.16033554 0.13681889]\n",
      " [0.47694707 0.00341952 0.7129787  0.09622478]\n",
      " [0.44369256 0.3684976  0.36041474 0.7140639 ]\n",
      " [0.11214304 0.739069   0.0445025  0.40412378]\n",
      " [0.54873455 0.24010873 0.14781737 0.05520809]\n",
      " [0.35333335 0.12355304 0.7780801  0.87558997]], shape=(32, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Layer\n",
    "\n",
    "\n",
    "class Time2Vec(Layer):\n",
    "    \"\"\"Time2Vector encoding layer\"\"\"\n",
    "\n",
    "    def __init__(self, kernel: int = 64, activation: str = \"sin\") -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kernel (int, optional): length of time vector representation. Defaults to 64\n",
    "            activation (str, optional): periodic activation for time encoding. Defaults to \"sin\".\n",
    "        Raises:\n",
    "            NotImplementedError: Non-supported activations\n",
    "        \"\"\"\n",
    "\n",
    "        # periodic components\n",
    "        if activation in [\"sin\", \"cos\"]:\n",
    "            activation = {\"sin\": tf.math.sin, \"cos\": tf.math.cos}[activation]\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"'{activation}' is an unsupported periodic activation.\"\n",
    "            )\n",
    "\n",
    "        super().__init__(trainable=True, name=\"Time2VecLayer_\" + activation.__name__)\n",
    "\n",
    "        self.k = kernel - 1\n",
    "        self.p_activation = activation\n",
    "\n",
    "    def build(self, input_shape: tuple) -> None:\n",
    "        \"\"\"method for building and initializing the weights for the tensor operations\n",
    "        Args:\n",
    "            input_shape (tuple): shape of the incoming tensor\n",
    "        \"\"\"\n",
    "        # Linear component\n",
    "        self.w_b = self.add_weight(\n",
    "            shape=(1, input_shape[1], 1), initializer=\"uniform\", trainable=True\n",
    "        )\n",
    "\n",
    "        self.b_b = self.add_weight(\n",
    "            shape=(1, input_shape[1], 1), initializer=\"uniform\", trainable=True\n",
    "        )\n",
    "\n",
    "        # Periodic components\n",
    "        self.freq = self.add_weight(\n",
    "            shape=(1, input_shape[1], self.k), initializer=\"uniform\", trainable=True\n",
    "        )\n",
    "\n",
    "        self.phase = self.add_weight(\n",
    "            shape=(1, input_shape[1], self.k), initializer=\"uniform\", trainable=True\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, **kwargs) -> tf.Tensor:\n",
    "\n",
    "        \"\"\"method to perform the layer operation\n",
    "        Args:\n",
    "            inputs (tf.Tensor): shape = (batch_size, feature_size)\n",
    "        Returns:\n",
    "            tf.Tensor: shape = (batch_size, feature_size, length of time vector representation)\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = tf.expand_dims(inputs, axis=-1)\n",
    "\n",
    "        # Linear components\n",
    "        lin = (\n",
    "            # Multiply each time dimension with the corresponding linear time component\n",
    "            tf.multiply(inputs, self.w_b)\n",
    "            # Bias component for each time dimension\n",
    "            + self.b_b\n",
    "        )\n",
    "\n",
    "        # Periodic components\n",
    "        # Multiply each time dimension (M, D, H, mins, etc.) with the corresponding frequency vector\n",
    "        per = tf.multiply(tf.tile(inputs, multiples=[1, 1, self.k]), self.freq)\n",
    "        # Phase vector for each time dimension\n",
    "        per = self.p_activation(per + self.phase)\n",
    "        return tf.concat([lin, per], -1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape: tuple) -> tuple:\n",
    "        \"\"\"computes the shape of output tensor\n",
    "        Args:\n",
    "            input_shape (tuple): shape of incoming tensor\n",
    "        Returns:\n",
    "            tuple: shape of outgoing tensor\n",
    "        \"\"\"\n",
    "        return input_shape[0], input_shape[1], self.k + 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 32 samples with time represented as %%M%%D%%H%%S\n",
    "    test_vector = tf.random.uniform(shape=(32, 4), dtype=tf.float32)\n",
    "    print(test_vector)\n",
    "    #xti = Time2Vec(16, \"sin\")(test_vector)\n",
    "    # Shape = (32, 4, 16)! 16 dimensional representation for each t-dimension\n",
    "    #print(xti.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:30:44.061126Z",
     "start_time": "2023-05-09T09:30:44.054783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " words_embedding (Embedding)  (None, 3, 50)            300       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300\n",
      "Trainable params: 300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, Flatten\n",
    "\n",
    "vocab_size = len(words_set)\n",
    "\n",
    "embedding_model = Sequential()\n",
    "embedding_model.add(Input(shape=(3,)))\n",
    "embedding_model.add(Embedding(input_dim= vocab_size + 1,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            name='words_embedding', trainable=True\n",
    "                            ))\n",
    "\n",
    "embedding_model.compile(optimizer='adam', loss='mse')\n",
    "embedding_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:11:48.448246Z",
     "start_time": "2023-05-09T09:11:48.422778Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 4, 50)]           0         \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 4, 64)             22272     \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 32)                9408      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      " model_10 (Functional)       (None, 4, 50)             25714     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,526\n",
      "Trainable params: 57,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, GRU, Dense, RepeatVector, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "input_shape = (4, 50)\n",
    "output_shape = (4, 50)\n",
    "\n",
    "latent_dim = 4\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "encoder = GRU(64, return_sequences=True)(inputs)\n",
    "encoder = GRU(32)(encoder)\n",
    "\n",
    "latent = Dense(latent_dim)(encoder)\n",
    "\n",
    "decoder_inputs = Input(shape=(latent_dim,))\n",
    "decoder = RepeatVector(input_shape[0])(decoder_inputs)\n",
    "decoder = GRU(32, return_sequences=True)(decoder)\n",
    "decoder = GRU(64, return_sequences=True)(decoder)\n",
    "decoder_outputs = TimeDistributed(Dense(output_shape[1]))(decoder)\n",
    "\n",
    "encoder_model = Model(inputs, latent)\n",
    "decoder_model = Model(decoder_inputs, decoder_outputs)\n",
    "\n",
    "model = Model(inputs, decoder_model(latent))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:53:57.608846Z",
     "start_time": "2023-05-09T09:53:56.350942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[73], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_vectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_vectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/keras/engine/training.py:1501\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1491\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cluster_coordinator \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1492\u001B[0m         tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mcoordinator\u001B[38;5;241m.\u001B[39mClusterCoordinator(\n\u001B[1;32m   1493\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy\n\u001B[1;32m   1494\u001B[0m         )\n\u001B[1;32m   1495\u001B[0m     )\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy\u001B[38;5;241m.\u001B[39mscope(), training_utils\u001B[38;5;241m.\u001B[39mRespectCompiledTrainableState(  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m     \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1499\u001B[0m ):\n\u001B[1;32m   1500\u001B[0m     \u001B[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001B[39;00m\n\u001B[0;32m-> 1501\u001B[0m     data_handler \u001B[38;5;241m=\u001B[39m \u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data_handler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1503\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1504\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1506\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1507\u001B[0m \u001B[43m        \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1513\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1515\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1516\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1518\u001B[0m     \u001B[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001B[39;00m\n\u001B[1;32m   1519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/keras/engine/data_adapter.py:1582\u001B[0m, in \u001B[0;36mget_data_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cluster_coordinator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _ClusterCoordinatorDataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1582\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/keras/engine/data_adapter.py:1262\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[1;32m   1259\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution \u001B[38;5;241m=\u001B[39m steps_per_execution\n\u001B[1;32m   1261\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m select_data_adapter(x, y)\n\u001B[0;32m-> 1262\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapter \u001B[38;5;241m=\u001B[39m \u001B[43madapter_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1263\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1264\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1265\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1266\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1268\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1272\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdistribution_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1277\u001B[0m strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n\u001B[1;32m   1279\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/keras/engine/data_adapter.py:349\u001B[0m, in \u001B[0;36mTensorLikeDataAdapter.__init__\u001B[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001B[0m\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m flat_dataset\n\u001B[1;32m    347\u001B[0m indices_dataset \u001B[38;5;241m=\u001B[39m indices_dataset\u001B[38;5;241m.\u001B[39mflat_map(slice_batch_indices)\n\u001B[0;32m--> 349\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mslice_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshuffle_batch\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/keras/engine/data_adapter.py:390\u001B[0m, in \u001B[0;36mTensorLikeDataAdapter.slice_inputs\u001B[0;34m(self, indices_dataset, inputs)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgrab_batch\u001B[39m(i, data):\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    387\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m d: tf\u001B[38;5;241m.\u001B[39mgather(d, i, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m), data\n\u001B[1;32m    388\u001B[0m     )\n\u001B[0;32m--> 390\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrab_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAUTOTUNE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001B[39;00m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001B[39;00m\n\u001B[1;32m    394\u001B[0m options \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mOptions()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2204\u001B[0m, in \u001B[0;36mDatasetV2.map\u001B[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001B[0m\n\u001B[1;32m   2202\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m MapDataset(\u001B[38;5;28mself\u001B[39m, map_func, preserve_cardinality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, name\u001B[38;5;241m=\u001B[39mname)\n\u001B[1;32m   2203\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2204\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mParallelMapDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2205\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2206\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2207\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2208\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2209\u001B[0m \u001B[43m      \u001B[49m\u001B[43mpreserve_cardinality\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2210\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5441\u001B[0m, in \u001B[0;36mParallelMapDataset.__init__\u001B[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001B[0m\n\u001B[1;32m   5439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_dataset \u001B[38;5;241m=\u001B[39m input_dataset\n\u001B[1;32m   5440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_inter_op_parallelism \u001B[38;5;241m=\u001B[39m use_inter_op_parallelism\n\u001B[0;32m-> 5441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func \u001B[38;5;241m=\u001B[39m \u001B[43mstructured_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mStructuredFunctionWrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5442\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5443\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transformation_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5445\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_legacy_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_legacy_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m deterministic \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5447\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deterministic \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001B[0m, in \u001B[0;36mStructuredFunctionWrapper.__init__\u001B[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[1;32m    264\u001B[0m       warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    265\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    266\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moption is set, this option does not apply to tf.data functions. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    267\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo force eager execution of tf.data functions, please use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    268\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    269\u001B[0m     fn_factory \u001B[38;5;241m=\u001B[39m trace_tf_function(defun_kwargs)\n\u001B[0;32m--> 271\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function \u001B[38;5;241m=\u001B[39m \u001B[43mfn_factory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# There is no graph to add in eager mode.\u001B[39;00m\n\u001B[1;32m    273\u001B[0m add_to_graph \u001B[38;5;241m&\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2610\u001B[0m, in \u001B[0;36mFunction.get_concrete_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2601\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_concrete_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   2602\u001B[0m   \u001B[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001B[39;00m\n\u001B[1;32m   2603\u001B[0m \n\u001B[1;32m   2604\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2608\u001B[0m \u001B[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001B[39;00m\n\u001B[1;32m   2609\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2610\u001B[0m   graph_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_concrete_function_garbage_collected\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2611\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2612\u001B[0m   graph_function\u001B[38;5;241m.\u001B[39m_garbage_collector\u001B[38;5;241m.\u001B[39mrelease()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   2613\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m graph_function\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2576\u001B[0m, in \u001B[0;36mFunction._get_concrete_function_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2574\u001B[0m   args, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2575\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m-> 2576\u001B[0m   graph_function, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2577\u001B[0m   seen_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[1;32m   2578\u001B[0m   captured \u001B[38;5;241m=\u001B[39m object_identity\u001B[38;5;241m.\u001B[39mObjectIdentitySet(\n\u001B[1;32m   2579\u001B[0m       graph_function\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39minternal_captures)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2760\u001B[0m, in \u001B[0;36mFunction._maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   2758\u001B[0m   \u001B[38;5;66;03m# Only get placeholders for arguments, not captures\u001B[39;00m\n\u001B[1;32m   2759\u001B[0m   args, kwargs \u001B[38;5;241m=\u001B[39m placeholder_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m-> 2760\u001B[0m graph_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_graph_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2762\u001B[0m graph_capture_container \u001B[38;5;241m=\u001B[39m graph_function\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39m_capture_func_lib  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   2763\u001B[0m \u001B[38;5;66;03m# Maintain the list of all captures\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2670\u001B[0m, in \u001B[0;36mFunction._create_graph_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   2665\u001B[0m missing_arg_names \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   2666\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (arg, i) \u001B[38;5;28;01mfor\u001B[39;00m i, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(missing_arg_names)\n\u001B[1;32m   2667\u001B[0m ]\n\u001B[1;32m   2668\u001B[0m arg_names \u001B[38;5;241m=\u001B[39m base_arg_names \u001B[38;5;241m+\u001B[39m missing_arg_names\n\u001B[1;32m   2669\u001B[0m graph_function \u001B[38;5;241m=\u001B[39m ConcreteFunction(\n\u001B[0;32m-> 2670\u001B[0m     \u001B[43mfunc_graph_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc_graph_from_py_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2671\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2672\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2673\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2675\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_signature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2678\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2679\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapture_by_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_capture_by_value\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   2680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_attributes,\n\u001B[1;32m   2681\u001B[0m     spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_spec,\n\u001B[1;32m   2682\u001B[0m     \u001B[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001B[39;00m\n\u001B[1;32m   2683\u001B[0m     \u001B[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001B[39;00m\n\u001B[1;32m   2684\u001B[0m     \u001B[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001B[39;00m\n\u001B[1;32m   2685\u001B[0m     \u001B[38;5;66;03m# ConcreteFunction.\u001B[39;00m\n\u001B[1;32m   2686\u001B[0m     shared_func_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   2687\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1153\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001B[0m\n\u001B[1;32m   1150\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1151\u001B[0m   deps_control_manager \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mNullContextmanager()\n\u001B[0;32m-> 1153\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m func_graph\u001B[38;5;241m.\u001B[39mas_default(), deps_control_manager \u001B[38;5;28;01mas\u001B[39;00m deps_ctx:\n\u001B[1;32m   1154\u001B[0m   current_scope \u001B[38;5;241m=\u001B[39m variable_scope\u001B[38;5;241m.\u001B[39mget_variable_scope()\n\u001B[1;32m   1155\u001B[0m   default_use_resource \u001B[38;5;241m=\u001B[39m current_scope\u001B[38;5;241m.\u001B[39muse_resource\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/framework/auto_control_deps.py:464\u001B[0m, in \u001B[0;36mAutomaticControlDependencies.__exit__\u001B[0;34m(self, unused_type, unused_value, unused_traceback)\u001B[0m\n\u001B[1;32m    461\u001B[0m resource_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[1;32m    462\u001B[0m \u001B[38;5;66;03m# Check for any resource inputs. If we find any, we update control_inputs\u001B[39;00m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# and last_write_to_resource.\u001B[39;00m\n\u001B[0;32m--> 464\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inp, resource_type \u001B[38;5;129;01min\u001B[39;00m _get_resource_inputs(op):\n\u001B[1;32m    465\u001B[0m   is_read \u001B[38;5;241m=\u001B[39m resource_type \u001B[38;5;241m==\u001B[39m ResourceType\u001B[38;5;241m.\u001B[39mREAD_ONLY\n\u001B[1;32m    466\u001B[0m   input_id \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mtensor_id(inp)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/framework/auto_control_deps.py:671\u001B[0m, in \u001B[0;36m_get_resource_inputs\u001B[0;34m(op)\u001B[0m\n\u001B[1;32m    665\u001B[0m saturated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m _acd_resource_resolvers_registry\u001B[38;5;241m.\u001B[39mlist():\n\u001B[1;32m    667\u001B[0m   \u001B[38;5;66;03m# Resolvers should return true if they are updating the list of\u001B[39;00m\n\u001B[1;32m    668\u001B[0m   \u001B[38;5;66;03m# resource_inputs.\u001B[39;00m\n\u001B[1;32m    669\u001B[0m   \u001B[38;5;66;03m# TODO(srbs): An alternate would be to just compare the old and new set\u001B[39;00m\n\u001B[1;32m    670\u001B[0m   \u001B[38;5;66;03m# but that may not be as fast.\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m   updated \u001B[38;5;241m=\u001B[39m \u001B[43m_acd_resource_resolvers_registry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m(op, reads, writes)\n\u001B[1;32m    672\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m updated:\n\u001B[1;32m    673\u001B[0m     \u001B[38;5;66;03m# Conservatively remove any resources from `reads` that are also writes.\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     reads \u001B[38;5;241m=\u001B[39m reads\u001B[38;5;241m.\u001B[39mdifference(writes)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/framework/registry.py:91\u001B[0m, in \u001B[0;36mRegistry.lookup\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlookup\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[1;32m     82\u001B[0m   \u001B[38;5;124;03m\"\"\"Looks up \"name\".\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;03m    LookupError: if \"name\" has not been registered.\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m---> 91\u001B[0m   name \u001B[38;5;241m=\u001B[39m \u001B[43mcompat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_registry:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_registry[name][_TYPE_TAG]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/robin/lib/python3.10/site-packages/tensorflow/python/util/compat.py:115\u001B[0m, in \u001B[0;36mas_str\u001B[0;34m(bytes_or_text, encoding)\u001B[0m\n\u001B[1;32m    111\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected binary or unicode string, got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m bytes_or_text)\n\u001B[0;32m--> 115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mas_str\u001B[39m(bytes_or_text, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    116\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m as_text(bytes_or_text, encoding)\n\u001B[1;32m    118\u001B[0m tf_export(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcompat.as_text\u001B[39m\u001B[38;5;124m'\u001B[39m)(as_text)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(input_vectors, input_vectors, epochs=10, batch_size=32, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:55:31.420272Z",
     "start_time": "2023-05-09T09:53:58.887208Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embedding_dict.keys(), key=lambda word: spatial.distance.euclidean(embedding_dict[word], embedding))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:12:17.818514Z",
     "start_time": "2023-05-09T09:12:17.812791Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input - Random passenger data: \n",
      "['business', 'madrid', 'zaragoza', array([2023,    6,    1,    7,   18,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0])]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Output - Passenger reconstruction: \n",
      "['business', 'madrid', 'barcelona'] [129, 6, 0, 10, 29, 0]\n"
     ]
    }
   ],
   "source": [
    "random_index = np.random.randint(0, len(input_vectors))\n",
    "random_vector = input_vectors[random_index]\n",
    "\n",
    "print(\"Input - Random passenger data: \")\n",
    "print(input_data[random_index])\n",
    "\n",
    "prediction = model.predict(np.array([random_vector]))[0]\n",
    "\n",
    "print(\"Output - Passenger reconstruction: \")\n",
    "decoded_prediction = [find_closest_embeddings(word_vector) for word_vector in prediction[:3]]\n",
    "print(decoded_prediction, [int(x) for x in prediction[-1][:6]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:13:51.524652Z",
     "start_time": "2023-05-09T09:13:51.459497Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T08:27:55.075132Z",
     "start_time": "2023-05-05T08:27:55.072445Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
